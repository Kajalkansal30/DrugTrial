#!/bin/bash

# Start Ollama server in the background
echo "üöÄ Starting Ollama server..."
ollama serve &

# Wait for Ollama to be ready
echo "‚è≥ Waiting for Ollama server to respond..."
until curl -s http://localhost:11434/api/tags > /dev/null; do
    sleep 2
done
echo "‚úÖ Ollama server is up!"

# Pull the model (configurable via OLLAMA_MODEL env var)
MODEL=${OLLAMA_MODEL:-"llama3.1"}
echo "üì• Ensuring ${MODEL} is available..."
ollama pull ${MODEL}

# Start the FastAPI application
echo "üåê Starting FastAPI application on port $PORT..."
exec uvicorn app:app --host 0.0.0.0 --port $PORT
