#!/bin/bash

# Start Ollama server in the background
echo "ğŸš€ Starting Ollama server..."
ollama serve &

# Wait for Ollama to be ready
echo "â³ Waiting for Ollama server to respond..."
until curl -s http://localhost:11434/api/tags > /dev/null; do
    sleep 2
done
echo "âœ… Ollama server is up!"

# Pull the model if not already present
echo "ğŸ“¥ Ensuring llama3.1 is available..."
ollama pull llama3.1

# Start the FastAPI application
echo "ğŸŒ Starting FastAPI application on port $PORT..."
exec uvicorn app:app --host 0.0.0.0 --port $PORT
